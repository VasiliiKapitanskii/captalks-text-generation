{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CAP Talks\n",
    "\n",
    "Разберем на примере генерации текста работу с Jupyter Notebook, нейронными сетями и сопутствующими библиотеками.\n",
    "\n",
    "Анализ текста является довольно сложной задачей для решения посредством обычного кода: огромное количество слов, зависимостей между ними не позволят создать хорошо расширяемый и общий алгоритм. Однако, для нейронной сети эта задача одна из самых типичных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Что будем \"читать\"?\n",
    "В качестве вводных данных для сети будем использовать несколько произведений Говарда Лавкрафта. Его стиль отличается малым количеством диалогов, зачастую написан в одном стиле, жанре, а значит будет проще научить сеть создавать текст, который хотя бы издалека будет напоминать авторский.\n",
    "\n",
    "- **Тень над Иннсмутом**\n",
    "- **Мифы Ктулху**\n",
    "- **Безымянный город** и другие\n",
    "\n",
    "![Lovecraft](img/lovecraft.jpg)\n",
    "\n",
    "### Почему не на русском?\n",
    "Английский язык гораздо проще для анализа, а следовательно и сгенерированный текст будет обладать меньшим количеством ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras — открытая нейросетевая библиотека, написанная на языке Python\n",
    "- Open-source\n",
    "- Работает поверх TensorFlow, Microsoft Cognitive Toolkit, Theano, or PlaidML\n",
    "- Нацелена на оперативную работу с сетями глубинного обучения\n",
    "- Спроектирована так, чтобы быть компактной, модульной и расширяемой\n",
    "\n",
    "## Жизненный цикл Keras\n",
    "![Keras Lifecycle](img/lifecycle.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Input, Flatten, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import spacy\n",
    "import codecs\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Чтение входного текста\n",
    "\n",
    "Необходимо максимально облегчить задачу нейронной сети: необходимо избавиться от ненужных символов и привести текст к нижнему регистру. Таким образом мы снизим размер словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '..\\Data'\n",
    "save_dir = '..\\Save'\n",
    "input_file = os.path.join(data_dir, 'lovecraft.txt')\n",
    "\n",
    "# reading the file\n",
    "with codecs.open(input_file, \"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "# creating words list\n",
    "nlp = spacy.load('en_core_web_sm') # spacy lib and English model to process the text\n",
    "doc = nlp(data)\n",
    "\n",
    "wordlist = []\n",
    "for word in doc:\n",
    "    if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "        wordlist.append(word.text.lower())\n",
    "\n",
    "print('First 10 words: ', wordlist[:10])\n",
    "print('Total words: ', len(wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Составление словаря\n",
    "\n",
    "Словарь будет содержать список слов и их индексы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "word_counts = collections.Counter(wordlist)\n",
    "\n",
    "# map words on indexes\n",
    "vocabulary_list = list(sorted(set(wordlist)))\n",
    "vocab = {x: i for i, x in enumerate(vocabulary_list)}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "    \n",
    "print('Vocabulary size: ', vocab_size)\n",
    "print('20 most common words: ', word_counts.most_common()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Создание списка последовательностей для обучения сети\n",
    "Нам необходимо 2 списка:\n",
    " - **sequences**: последовательности слов, которые будем использовать для обучения сети\n",
    " - **next_words**: список следующих слов, основанных на **sequences**\n",
    " \n",
    "30 первых слов из нашего текста будет первой последовательностью. 31 слово заносим в список \"следующих\", **next_words**.\n",
    "Далее, переходя на одно слово вперед по тексту, мы создадим следующую последовательность, и так до конца текста.\n",
    "\n",
    "![Sequences](img/sequences.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sequences_length = 30\n",
    "next_words = []\n",
    "sequences = []\n",
    "\n",
    "for i in range(0, len(wordlist) - sequences_length, 1):\n",
    "    sequences.append(wordlist[i: i + sequences_length])\n",
    "    next_words.append(wordlist[i + sequences_length])\n",
    "\n",
    "print('Sequences total: ', len(sequences))\n",
    "print('First sequence: ', sequences[0])\n",
    "print('First sequence next word: ', next_words[0])\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Так как сеть работает с числовыми значениями, необходимо перевести каждое слово на соответствующее ему значение индекса и провести векторизацию.\n",
    "\n",
    "Получим следующие матрицы:\n",
    " - X : матрица векторов 'последовательностей' со следующими измерениями:\n",
    "     - массив последовательностей,\n",
    "     - массив слов в последовательности,\n",
    "     - словарь для каждого слова.\n",
    " - Y : матрица векторов 'следующих' слов со следующими измерениями:\n",
    "     - массив последовательностей,\n",
    "     - словарь для каждого слова.\n",
    "     \n",
    "Для каждого слова мы определяем его индекс в словаре и устанавливаем True в матрице на его позиции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# vectorization\n",
    "X = np.zeros((len(sequences), sequences_length, vocab_size), dtype = np.bool)\n",
    "Y = np.zeros((len(sequences), vocab_size), dtype = np.bool)\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, word in enumerate(sequence):\n",
    "        X[i, t, vocab[word]] = 1\n",
    "    Y[i, vocab[next_words[i]]] = 1\n",
    "\n",
    "print('X consists of', len(sequences), 'sequences, each sequence contains', len(X[0]), 'words')\n",
    "print('Y consists of', len(Y), '\"next\" words')\n",
    "print('Each word represented as True in vocabulary of', len(X[0][0]), 'elements')\n",
    "\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Выбор типа нейронной сети\n",
    "\n",
    "Для начала необходимо выбрать тип нейронной сети.\n",
    "\n",
    "Нейронные сети прямого распространения (feed-forward) не содержат петель, то есть информация передается по сети только вперед, не имея возможности повлиять на ввод.\n",
    "\n",
    "![Feed-forward neural network](img/fnn.png)\n",
    "<h3 align=\"center\"><i>Feed-forward neural network</i></h3>\n",
    "\n",
    "Такая сеть нам не подходит поскольку мы будем иметь дело с данными, имеющими характерную последовательность - предложения, а значит хотелось бы иметь возможность \"запоминать\" выходные значения нейронов и влиять ими на входные данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Такие сети называются **рекуррентными**. Они имеют нейроны, которые срабатывают в течение некоторого ограниченного периода времени, прежде чем перейти в состояние покоя. Это срабатывание может стимулировать другие нейроны, которые могут срабатывать немного позже, также в течение ограниченного времени. Это вызывает еще большее количество нейронов, и поэтому со временем мы получаем каскад нейронов, влияющих друг на друга.\n",
    "\n",
    "![Recurrent neural network](img/rnn.png)\n",
    "<h3 align=\"center\"><i>Recurrent neural network</i></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор рекуррентной сети\n",
    "\n",
    "Мы будем использовать архитектурную разновидность рекуррентной нейронной сети - **Bidirectional Long Short-Term Memory, LSTM**. Архитектура двунаправленной долгой краткосрочной памяти отличается более быстрой сходимостью по сравнению с однонаправленной, а также показала большую точность в тестах.\n",
    "\n",
    "**LSTM** обычно используют в аналитических задачах на прогнозирование временных рядов. Прогнозирование отлично подходит под описание нашей задачи - сеть должна выбирать наиболее вероятное слово, основываясь на последовательности слов, заданной пользователем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Создание нейронной сети\n",
    "\n",
    "Наша сеть будет иметь следующие характеристики и архитектуру:\n",
    "\n",
    " - последовательная архитектура определяет общую конструкцию сети - линейн\n",
    " - архитектура bidirectional LSTM размером 256 с функцией активации ReLU\n",
    " - dropout с rate 0,6 (помогает предотвратить переобучение (overfitting)) \n",
    " - классический полносвязный слой размером в количество слов в словаре с активацией Softmax для вывода вероятности слова\n",
    " - оптимизатор ADAM\n",
    " \n",
    " ![Network architecture](img/nn.png)\n",
    "<h4 align=\"center\"><i>Network architecture</i></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  ![Dropout](img/dropout.png)\n",
    "<h4 align=\"center\"><i>Dropout</i></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(256, activation = \"relu\"), \n",
    "                        input_shape = (sequences_length, vocab_size)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr = 0.001))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обучение нейронной сети\n",
    "\n",
    "- Тренируем 70 эпох: проход по ВСЕМ тренировочным данным + backprop (epochs = 70)\n",
    "- Сохраняем сеть каждые 10 эпох в отдельный файл (period = 10, ~230 MB каждый)\n",
    "- Обучаем на 128 сэмплах (последовательностях слов) за одну итерацию, после чего обновляем градиент (batch_size = 128)\n",
    "- Каждую эпоху перемешиваем тренировочные данные (shuffle = True)\n",
    "- Обучаем сеть на 90% тренировочных данных и 10% тестовых (validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "callbacks=[ModelCheckpoint(\n",
    "               filepath = save_dir + \"/\" + 'lovecraft-{epoch:02d}.hdf5',\n",
    "               verbose = 0,\n",
    "               mode = 'auto',\n",
    "               period = 10)]\n",
    "\n",
    "history = model.fit(\n",
    "             X[:500], # remove [:500] to use whole dataset\n",
    "             Y[:500], # remove [:500] to use whole dataset\n",
    "             batch_size = 128,\n",
    "             shuffle = True,\n",
    "             epochs = 1, # 70\n",
    "             callbacks = callbacks,\n",
    "             validation_split = 0.1)\n",
    "\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.save(save_dir + \"/\" + 'lovecraft-final.hdf5')\n",
    "\n",
    "print('Model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Генерация текста\n",
    "\n",
    "Обучение закончено, мы готовы генерировать текст. Для этого необходимо:\n",
    "\n",
    "- создать текст длиной 30 слов (требуется сетью для последовательностей)\n",
    "- создать прогноз на следующее слово\n",
    "- отбросить первое слово и добавить спрогнозированное\n",
    "- повторить N раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "seed_sentences = (\"cthulhu is the most frightening thing in the world . \"\n",
    "                  \"one day the cult will find the hideous citadel of terrifying creature \"\n",
    "                  \"and emerge it from the dark sea . \")\n",
    "\n",
    "print('Generating text with the following seed: \"' + seed_sentences + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def generateText(input_sentences, words_number):\n",
    "    generated = input_sentences\n",
    "    sentences_splitted = input_sentences.split()\n",
    "    \n",
    "    # generate the text\n",
    "    for i in range(words_number):\n",
    "        \n",
    "        # vectorize the input\n",
    "        x = np.zeros((1, sequences_length, vocab_size))\n",
    "        for t, word in enumerate(sentences_splitted):\n",
    "            x[0, t, vocab[word]] = 1.\n",
    "    \n",
    "        # calculate next word\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = np.argmax(preds)\n",
    "        next_word = vocabulary_list[next_index]\n",
    "    \n",
    "        # add the next word to the text\n",
    "        generated += \" \" + next_word\n",
    "        \n",
    "        # shift the sentence by one, add the next word at its end\n",
    "        sentences_splitted = sentences_splitted[1:] + [next_word]\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def prettify(generated):\n",
    "    punctuation = [' ', ',', '.', '!', '?', ';', '\\'']\n",
    "    pretty = ''\n",
    "    \n",
    "    for i, ltr in enumerate(generated):\n",
    "        if i == 0 or generated[i - 2] == '.':\n",
    "            pretty += ltr.upper()\n",
    "            continue\n",
    "        if ltr == ' ' and generated[i + 1] in punctuation:\n",
    "            continue\n",
    "        pretty += ltr\n",
    "    return pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "generated = generateText(seed_sentences, 100)\n",
    "prettyfied = prettify(generated)\n",
    "\n",
    "print(prettyfied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(save_dir + \"/\" + 'lovecraft_70epochs.hdf5')\n",
    "\n",
    "print('Loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "generated = generateText(seed_sentences, 100)\n",
    "prettyfied = prettify(generated)\n",
    "\n",
    "for i, sentence in enumerate(prettyfied.split('. ')):\n",
    "    print(sentence + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![Meme](img/meme.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
